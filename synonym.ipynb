{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym Augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class WordNet:\n",
    "    def __init__(self, lang='eng', is_synonym=True):\n",
    "        self.lang = lang\n",
    "        self.is_synonym = is_synonym\n",
    "        self.model = self.read()\n",
    "\n",
    "    def read(self):\n",
    "        try:\n",
    "            wordnet.synsets('testing')\n",
    "            return wordnet\n",
    "        except LookupError:\n",
    "            nltk.download('wordnet')\n",
    "            nltk.download('omw-1.4')\n",
    "            return wordnet\n",
    "\n",
    "    def predict(self, word, pos=None):\n",
    "        results = []\n",
    "        for synonym in self.model.synsets(word, pos=pos, lang=self.lang):\n",
    "            for lemma in synonym.lemmas(lang=self.lang):\n",
    "                if self.is_synonym:\n",
    "                    results.append(lemma.name())\n",
    "                else:\n",
    "                    for antonym in lemma.antonyms():\n",
    "                        results.append(antonym.name())\n",
    "        return results\n",
    "\n",
    "    @classmethod\n",
    "    def pos_tag(cls, tokens):\n",
    "        try:\n",
    "            results = nltk.pos_tag(tokens)\n",
    "        except LookupError:\n",
    "            nltk.download('averaged_perceptron_tagger')\n",
    "            results = nltk.pos_tag(tokens)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartOfSpeech:\n",
    "    NOUN = 'noun'\n",
    "    VERB = 'verb'\n",
    "    ADJECTIVE = 'adjective'\n",
    "    ADVERB = 'adverb'\n",
    "\n",
    "    pos2con = {\n",
    "        'n': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "        'v': ['VB', 'VBD', 'VBG', 'VBN', 'VBZ', 'VBP'],\n",
    "        'a': ['JJ', 'JJR', 'JJS', 'IN'],\n",
    "        's': ['JJ', 'JJR', 'JJS', 'IN'],  # Adjective Satellite\n",
    "        'r': ['RB', 'RBR', 'RBS'],\n",
    "    }\n",
    "\n",
    "    con2pos = {}\n",
    "    poses = []\n",
    "    for key, values in pos2con.items():\n",
    "        poses.extend(values)\n",
    "        for value in values:\n",
    "            if value not in con2pos:\n",
    "                con2pos[value] = []\n",
    "            con2pos[value].append(key)\n",
    "\n",
    "    @staticmethod\n",
    "    def pos2constituent(pos):\n",
    "        return PartOfSpeech.pos2con.get(pos, [])\n",
    "\n",
    "    @staticmethod\n",
    "    def constituent2pos(con):\n",
    "        return PartOfSpeech.con2pos.get(con, [])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pos():\n",
    "        return PartOfSpeech.poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ppdb_model(dict_path, force_reload=False):\n",
    "    global PPDB_MODEL\n",
    "\n",
    "    model_name = os.path.basename(dict_path)\n",
    "    if model_name in PPDB_MODEL and not force_reload:\n",
    "        return PPDB_MODEL[model_name]\n",
    "\n",
    "    model = nmw.Ppdb(dict_path)\n",
    "    PPDB_MODEL[model_name] = model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.words import WordsUtil\n",
    "import random\n",
    "\n",
    "def augment_with_synonyms(data, aug_src='wordnet', model_path=None, lang='eng',\n",
    "                          aug_min=1, aug_max=10, aug_p=0.3, stopwords=None,\n",
    "                          tokenizer=None, reverse_tokenizer=None, stopwords_regex=None,\n",
    "                          force_reload=False, verbose=0):\n",
    "    if not data or not data.strip():\n",
    "        return data\n",
    "\n",
    "    model = WordNet(lang=lang) if aug_src == 'wordnet' else None\n",
    "    if model is None:\n",
    "        raise ValueError('aug_src must be either `wordnet` or `ppdb`.')\n",
    "\n",
    "    change_seq = 0\n",
    "    tokenizer = tokenizer or str.split\n",
    "    reverse_tokenizer = reverse_tokenizer or ' '.join\n",
    "    doc = WordsUtil(data, tokenizer(data))\n",
    "\n",
    "    original_tokens = doc.get_original_tokens()\n",
    "    pos = model.pos_tag(original_tokens)\n",
    "    stopwords = stopwords or []\n",
    "\n",
    "    def skip_aug(token_idxes, tokens):\n",
    "        results = []\n",
    "        for token_idx in token_idxes:\n",
    "            if tokens[token_idx][1] in ['DT']:\n",
    "                continue\n",
    "\n",
    "            word_poses = PartOfSpeech.constituent2pos(tokens[token_idx][1])\n",
    "            if aug_src == 'ppdb' and not word_poses:\n",
    "                continue\n",
    "\n",
    "            if word_poses and not any(model.predict(tokens[token_idx][0], pos=pos) for pos in word_poses):\n",
    "                continue\n",
    "\n",
    "            results.append(token_idx)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _get_aug_idxes(tokens):\n",
    "        aug_cnt = min(len(tokens), int(len(tokens) * aug_p)) if aug_p else aug_max\n",
    "        word_idxes = [i for i in range(len(tokens)) if i not in stopwords]\n",
    "        word_idxes = skip_aug(word_idxes, tokens)\n",
    "\n",
    "        return random.sample(word_idxes, aug_cnt) if word_idxes else []\n",
    "\n",
    "    aug_idxes = _get_aug_idxes(pos)\n",
    "    if not aug_idxes:\n",
    "        return data\n",
    "\n",
    "    for aug_idx in aug_idxes:\n",
    "        original_token = original_tokens[aug_idx]\n",
    "        word_poses = PartOfSpeech.constituent2pos(pos[aug_idx][1])\n",
    "        candidates = sum((model.predict(pos[aug_idx][0], pos=word_pos) for word_pos in word_poses), [])\n",
    "\n",
    "        candidates = [c for c in candidates if c.lower() != original_token.lower()]\n",
    "\n",
    "        if candidates:\n",
    "            substitute_token = random.choice(candidates).lower()\n",
    "            if aug_idx == 0:\n",
    "                substitute_token = substitute_token.capitalize()\n",
    "\n",
    "            change_seq += 1\n",
    "            doc.add_change_log(aug_idx, new_token=substitute_token, action='substitute', change_seq=change_seq)\n",
    "\n",
    "    return reverse_tokenizer(doc.get_augmented_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nimble brown fox jumps all_over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "augmented_text = augment_with_synonyms(sample_text, aug_src='wordnet', aug_p=0.3)\n",
    "print(augmented_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002-egHm7VgB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
